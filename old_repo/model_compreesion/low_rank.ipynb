{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functin Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ 1) Low-Rank Decomposition of Conv1d(kernel_size=1) ================\n",
    "def low_rank_conv1d(conv_layer, rank):\n",
    "    \"\"\"\n",
    "    Perform low-rank decomposition on Conv1d(kernel_size=1):\n",
    "    Original weight.shape = (out_channels, in_channels, 1).\n",
    "    After decomposition, replace with: Sequential(Conv1d(in_channels, rank, 1), ReLU, Conv1d(rank, out_channels, 1)).\n",
    "    \"\"\"\n",
    "    W = conv_layer.weight.data  # (out_channels, in_channels, 1)\n",
    "    out_channels, in_channels, kernel_size = W.shape\n",
    "    \n",
    "    if kernel_size != 1:\n",
    "        # If not a 1Ã—1 conv, skip or raise an error\n",
    "        raise ValueError(\"Currently only support Conv1d with kernel_size=1.\")\n",
    "\n",
    "    # 1) Reshape weight to (out_channels, in_channels)\n",
    "    W_2d = W.view(out_channels, in_channels)\n",
    "\n",
    "    # 2) Perform SVD decomposition\n",
    "    U, S, Vh = torch.linalg.svd(W_2d, full_matrices=False)\n",
    "\n",
    "    # 3) Truncate to rank\n",
    "    U = U[:, :rank]        # (out_channels, rank)\n",
    "    S = S[:rank]           # (rank,)\n",
    "    Vh = Vh[:rank, :]      # (rank, in_channels)\n",
    "\n",
    "    # 4) Create two smaller conv layers\n",
    "    conv1 = nn.Conv1d(in_channels, rank, kernel_size=1, bias=False)\n",
    "    conv2 = nn.Conv1d(rank, out_channels, kernel_size=1, bias=False)\n",
    "\n",
    "    # 5) Assign weights\n",
    "    conv1.weight.data = Vh.unsqueeze(2)            # (rank, in_channels, 1)\n",
    "    conv2.weight.data = (U * S).unsqueeze(2)       # (out_channels, rank, 1)\n",
    "\n",
    "    # 6) Replace the original layer with Sequential (optionally add activation function)\n",
    "    return nn.Sequential(conv1, nn.ReLU(), conv2)\n",
    "\n",
    "\n",
    "# ================ 2) Low-Rank Decomposition of SpectralConv (DenseTensor) ================\n",
    "def low_rank_spectral_conv(spectral_layer, rank):\n",
    "    \"\"\"\n",
    "    Approximate the weight of SpectralConv (DenseTensor(shape=[C_out, C_in, Nx, Ny])).\n",
    "    This demonstrates an approach to performing truncated SVD on [C_out, C_in] dimensions (for reference).\n",
    "    You need to modify this according to the actual DenseTensor API:\n",
    "      - How to retrieve the underlying torch.Tensor?\n",
    "      - How to write back to DenseTensor?\n",
    "    \"\"\"\n",
    "    # -- Assume spectral_layer.weight is a DenseTensor --\n",
    "    # (1) Retrieve the actual torch.Tensor\n",
    "    #    (The following names are purely examples, adjust based on the actual implementation of DenseTensor)\n",
    "    if not hasattr(spectral_layer.weight, \"to_tensor\"):\n",
    "        print(f\"[Warning] DenseTensor has no 'to_tensor()' method. Skipped.\")\n",
    "        return\n",
    "    \n",
    "    W_torch = spectral_layer.weight.to_tensor()  # => Shape [C_out, C_in, Nx, Ny], type: torch.Tensor\n",
    "    \n",
    "    C_out, C_in, Nx, Ny = W_torch.shape\n",
    "\n",
    "    # (2) Perform 2D SVD only on channel dimensions, treating Nx, Ny as batch/extra dimensions\n",
    "    #     Simplest approach: reshape -> (C_out*C_in, Nx*Ny)\n",
    "    #     Perform a 2D SVD -> Truncate to rank\n",
    "    W_2d = W_torch.view(C_out*C_in, Nx*Ny)  # => (C_out*C_in, Nx*Ny)\n",
    "\n",
    "    U, S, Vh = torch.linalg.svd(W_2d, full_matrices=False)\n",
    "    # U.shape: (C_out*C_in, C_out*C_in)\n",
    "    # S.shape: (min(C_out*C_in, Nx*Ny),)\n",
    "    # Vh.shape: (min(C_out*C_in, Nx*Ny), Nx*Ny)\n",
    "\n",
    "    # Truncate to rank, ensuring it does not exceed the minimum dimension of U, Vh\n",
    "    max_rank = min(rank, U.shape[1], Vh.shape[0])  \n",
    "    U = U[:, :max_rank]\n",
    "    S = S[:max_rank]\n",
    "    Vh = Vh[:max_rank, :]\n",
    "\n",
    "    # Approximate: W_2d_low = U * S * Vh\n",
    "    #   (C_out*C_in, max_rank) x (max_rank,) x (max_rank, Nx*Ny)\n",
    "    # First multiply (U * S), then multiply Vh\n",
    "    U_S = U * S.unsqueeze(0)  # Broadcast\n",
    "    W_2d_low = U_S @ Vh       # => (C_out*C_in, Nx*Ny)\n",
    "\n",
    "    # Reshape back to original shape\n",
    "    W_low = W_2d_low.view(C_out, C_in, Nx, Ny)\n",
    "\n",
    "    # (3) Write back to DenseTensor\n",
    "    # Assume DenseTensor has a from_tensor() or set_tensor() method\n",
    "    if hasattr(spectral_layer.weight, \"from_tensor\"):\n",
    "        spectral_layer.weight.from_tensor(W_low)\n",
    "    else:\n",
    "        print(\"[Warning] No method to write back to DenseTensor. Skipped.\")\n",
    "\n",
    "\n",
    "# ================ 3) Iterate Through Model and Apply Low-Rank Decomposition ================\n",
    "def apply_low_rank_decomposition(model, rank=16):\n",
    "    \"\"\"\n",
    "    1) Apply low-rank decomposition to all Conv1d(kernel_size=1) layers (replacing with two smaller Conv1d layers)\n",
    "    2) Apply low-rank decomposition to DenseTensor in SpectralConv\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        # --- (a) Conv1d ---\n",
    "        if isinstance(module, nn.Conv1d):\n",
    "            # Check kernel_size\n",
    "            if module.kernel_size == (1,):\n",
    "                print(f\"[LowRank] Replacing 1x1 Conv1d at: {name}\")\n",
    "                # Find parent module\n",
    "                parent_name = \".\".join(name.split(\".\")[:-1])\n",
    "                child_name = name.split(\".\")[-1]\n",
    "\n",
    "                # Construct low-rank replacement layer\n",
    "                new_layer = low_rank_conv1d(module, rank)\n",
    "                # Replace new_layer in parent\n",
    "                if parent_name:\n",
    "                    parent_module = dict(model.named_modules())[parent_name]\n",
    "                    setattr(parent_module, child_name, new_layer)\n",
    "                else:\n",
    "                    setattr(model, name, new_layer)\n",
    "\n",
    "        # --- (b) SpectralConv (DenseTensor) ---\n",
    "        # Need to determine based on actual class name.\n",
    "        # Example: if isinstance(module, SpectralConvClass): ...\n",
    "        # Here, we simply check if \"SpectralConv\" is in the class name\n",
    "        elif \"SpectralConv\" in type(module).__name__:\n",
    "            # DenseTensor is typically stored as module.weight: DenseTensor(...)\n",
    "            # Verify first\n",
    "            if hasattr(module, \"weight\"):\n",
    "                print(f\"[LowRank] Approximating DenseTensor at: {name} shape={module.weight.shape}\")\n",
    "                low_rank_spectral_conv(module, rank)\n",
    "            else:\n",
    "                print(f\"[Warning] Found SpectralConv but no 'weight' attribute: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(file_path):\n",
    "    size_in_bytes = os.path.getsize(file_path)\n",
    "    size_in_mb = size_in_bytes / (1024 ** 2)  # to MB\n",
    "    return size_in_mb\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'e:/UCLCS/UCL Project/Neural Operator/COMP0031-Model-Compression-on-Neural-Operator'\n",
    "model_dir = os.path.join(base_dir, 'models', 'darcy_small.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Evave\\AppData\\Local\\Temp\\ipykernel_8780\\4067815592.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_dir)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FNO(\n",
       "  (positional_embedding): GridEmbeddingND()\n",
       "  (fno_blocks): FNOBlocks(\n",
       "    (convs): ModuleList(\n",
       "      (0-3): 4 x SpectralConv(\n",
       "        (weight): DenseTensor(shape=torch.Size([32, 32, 16, 9]), rank=None)\n",
       "      )\n",
       "    )\n",
       "    (fno_skips): ModuleList(\n",
       "      (0-3): 4 x Flattened1dConv(\n",
       "        (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (channel_mlp): ModuleList(\n",
       "      (0-3): 4 x ChannelMLP(\n",
       "        (fcs): ModuleList(\n",
       "          (0): Conv1d(32, 16, kernel_size=(1,), stride=(1,))\n",
       "          (1): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (channel_mlp_skips): ModuleList(\n",
       "      (0-3): 4 x SoftGating()\n",
       "    )\n",
       "  )\n",
       "  (lifting): ChannelMLP(\n",
       "    (fcs): ModuleList(\n",
       "      (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
       "      (1): Conv1d(64, 32, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (projection): ChannelMLP(\n",
       "    (fcs): ModuleList(\n",
       "      (0): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
       "      (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(model_dir)\n",
    "original_size = get_model_size(model_dir)\n",
    "original_params = count_parameters(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LowRank] Approximating DenseTensor at: fno_blocks.convs.0 shape=torch.Size([32, 32, 16, 9])\n",
      "[LowRank] Approximating DenseTensor at: fno_blocks.convs.1 shape=torch.Size([32, 32, 16, 9])\n",
      "[LowRank] Approximating DenseTensor at: fno_blocks.convs.2 shape=torch.Size([32, 32, 16, 9])\n",
      "[LowRank] Approximating DenseTensor at: fno_blocks.convs.3 shape=torch.Size([32, 32, 16, 9])\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.fno_skips.0.conv.0\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.fno_skips.0.conv.2\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.fno_skips.1.conv.0\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.fno_skips.1.conv.2\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.fno_skips.2.conv.0\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.fno_skips.2.conv.2\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.fno_skips.3.conv.0\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.fno_skips.3.conv.2\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.channel_mlp.0.fcs.0.0\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.channel_mlp.0.fcs.0.2\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.channel_mlp.0.fcs.1.0\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.channel_mlp.0.fcs.1.2\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.channel_mlp.1.fcs.0.0\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.channel_mlp.1.fcs.0.2\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.channel_mlp.1.fcs.1.0\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.channel_mlp.1.fcs.1.2\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.channel_mlp.2.fcs.0.0\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.channel_mlp.2.fcs.0.2\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.channel_mlp.2.fcs.1.0\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.channel_mlp.2.fcs.1.2\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.channel_mlp.3.fcs.0.0\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.channel_mlp.3.fcs.0.2\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.channel_mlp.3.fcs.1.0\n",
      "[LowRank] Replacing 1x1 Conv1d at: fno_blocks.channel_mlp.3.fcs.1.2\n",
      "[LowRank] Replacing 1x1 Conv1d at: lifting.fcs.0.0\n",
      "[LowRank] Replacing 1x1 Conv1d at: lifting.fcs.0.2\n",
      "[LowRank] Replacing 1x1 Conv1d at: lifting.fcs.1.0\n",
      "[LowRank] Replacing 1x1 Conv1d at: lifting.fcs.1.2\n",
      "[LowRank] Replacing 1x1 Conv1d at: projection.fcs.0.0\n",
      "[LowRank] Replacing 1x1 Conv1d at: projection.fcs.0.2\n",
      "[LowRank] Replacing 1x1 Conv1d at: projection.fcs.1.0\n",
      "[LowRank] Replacing 1x1 Conv1d at: projection.fcs.1.2\n"
     ]
    }
   ],
   "source": [
    "apply_low_rank_decomposition(model, rank=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model parameter number: 602977\n",
      "Low-Rank model parameter number: 591076\n",
      "parameter compression rate: 1.97%\n"
     ]
    }
   ],
   "source": [
    "low_rank_params = count_parameters(model)\n",
    "print(f\"Original model parameter number: {original_params}\")\n",
    "print(f\"Low-Rank model parameter number: {low_rank_params}\")\n",
    "print(f\"parameter compression rate: {(1 - low_rank_params / original_params) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FNO(\n",
       "  (positional_embedding): GridEmbeddingND()\n",
       "  (fno_blocks): FNOBlocks(\n",
       "    (convs): ModuleList(\n",
       "      (0-3): 4 x SpectralConv(\n",
       "        (weight): DenseTensor(shape=torch.Size([32, 32, 16, 9]), rank=None)\n",
       "      )\n",
       "    )\n",
       "    (fno_skips): ModuleList(\n",
       "      (0-3): 4 x Flattened1dConv(\n",
       "        (conv): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): Conv1d(32, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (1): ReLU()\n",
       "            (2): Conv1d(8, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          )\n",
       "          (1): ReLU()\n",
       "          (2): Sequential(\n",
       "            (0): Conv1d(1, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (1): ReLU()\n",
       "            (2): Conv1d(8, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (channel_mlp): ModuleList(\n",
       "      (0-3): 4 x ChannelMLP(\n",
       "        (fcs): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Conv1d(32, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(8, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "            (1): ReLU()\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(1, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(8, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Conv1d(16, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(8, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "            (1): ReLU()\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(1, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "              (1): ReLU()\n",
       "              (2): Conv1d(8, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (channel_mlp_skips): ModuleList(\n",
       "      (0-3): 4 x SoftGating()\n",
       "    )\n",
       "  )\n",
       "  (lifting): ChannelMLP(\n",
       "    (fcs): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv1d(3, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv1d(8, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): Sequential(\n",
       "          (0): Conv1d(1, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv1d(8, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv1d(64, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv1d(8, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): Sequential(\n",
       "          (0): Conv1d(1, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv1d(8, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projection): ChannelMLP(\n",
       "    (fcs): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv1d(32, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv1d(8, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): Sequential(\n",
       "          (0): Conv1d(1, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv1d(8, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv1d(64, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv1d(8, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (1): ReLU()\n",
       "        (2): Sequential(\n",
       "          (0): Conv1d(1, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv1d(8, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
